{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_rows = 999\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = '../models/w2v_glove_300.txt'\n",
    "word2vec_output_file = 'w2v.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('heart', 0.4828336238861084), ('cardiac', 0.45815250277519226), ('debilitating', 0.4523412585258484), ('infarction', 0.4387247562408447), ('illness', 0.4336370527744293), ('complications', 0.43345579504966736)]\n"
     ]
    }
   ],
   "source": [
    "w1 =\"stroke\"\n",
    "print(model.most_similar(positive = w1, topn = 6))\n",
    "#model.most_similar_cosmul(positive=['hepatoma', 'brain'], negative=['liver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "file = \"../data/stopwords.txt\"\n",
    "with open(file) as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(sentence):\n",
    "    word_list = TreebankWordTokenizer().tokenize(sentence)\n",
    "    word_list = [word for word in word_list if word not in stop_words]\n",
    "    word_vectors = []\n",
    "    for x in word_list:\n",
    "        try:\n",
    "            word_vectors.append(model[x])\n",
    "        except KeyError:\n",
    "            None    \n",
    "    return sum(word_vectors)/len(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prepartitioned train/test sets\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "train = pd.read_csv(\"../data/AMIA_train_set.csv\")\n",
    "\n",
    "test['vec'] = [sentence_vector(x) for x in test.text]\n",
    "train['vec'] = [sentence_vector(x) for x in train.text]\n",
    "\n",
    "# load full data set\n",
    "frames = [test, train]\n",
    "df = pd.concat(frames)\n",
    "df.expansion.unique()\n",
    "\n",
    "train_grouped_abbr = train.groupby('abbrev')\n",
    "test_grouped_abbr = test.groupby('abbrev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameGroupBy' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8cc20f877247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_grouped_abbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         raise AttributeError(\"%r object has no attribute %r\" %\n\u001b[0;32m--> 536\u001b[0;31m                              (type(self).__name__, attr))\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     @Substitution(klass='GroupBy',\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameGroupBy' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different abbreviations.\n",
    "clf_type = 'mlp'\n",
    "for abbr in train.abbrev.unique():\n",
    "\n",
    "    train_abbr = train_grouped_abbr.get_group(abbr)\n",
    "    test_abbr = test_grouped_abbr.get_group(abbr)\n",
    "\n",
    "    X_train = np.array(list(train_abbr.vec))\n",
    "    y_train = train_abbr.expansion\n",
    "\n",
    "    X_test = np.array(list(test_abbr.vec))\n",
    "    y_test = test_abbr.expansion\n",
    "\n",
    "    if clf_type == 'svm':\n",
    "        # set up SVM\n",
    "        clf = SVC(C=1.0, kernel='linear', degree=1, probability=True).fit(X_train, y_train)\n",
    "\n",
    "    elif clf_type == 'logistic':\n",
    "        clf = LogisticRegression().fit(X_train, y_train)\n",
    "        \n",
    "    elif clf_type == 'mlp':\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500).fit(X_train, y_train)\n",
    "        \n",
    "    pred = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, pred, labels=list(set(df.expansion)))\n",
    "    print()\n",
    "    print(\"##\" * 20)\n",
    "    print(\" \" * 20 + abbr)\n",
    "    print(\"##\" * 20)\n",
    "\n",
    "    print(classification_report(y_test, pred))\n",
    "    print()\n",
    "    print(f'examples (first 5 cases)\\t\\t\\t\\t\\t\\ttrue_abbr\\t\\t\\tpred_abbr')\n",
    "\n",
    "    # Print first 5 cases\n",
    "    i = 0\n",
    "    for input_row, true_abbr, pred_abbr in zip(train_abbr.iterrows(), y_test, pred):\n",
    "\n",
    "        sn_start = max(input_row[1].start - 25, 0)\n",
    "        sn_end = min(input_row[1].end + 25, len(input_row[1].text))\n",
    "\n",
    "        example_text = input_row[1].text[sn_start: sn_end]\n",
    "        print(f'... {example_text} ...\\t{true_abbr:<35}\\t{pred_abbr}')\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_test(X, y):\n",
    "    \n",
    "    # bagging test\n",
    "    seed = 1075\n",
    "    np.random.seed(seed)\n",
    "    max_samples = [0.05, 0.1, 0.2, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "    # Define classifiers:\n",
    "    svc = SVC(C=1.0, degree=1) \n",
    "    logistic = LogisticRegression()\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "\n",
    "    #for i in range(1, 11):\n",
    "    #n = i*0.1\n",
    "    \n",
    "    for n in max_samples:\n",
    "        print('max samples:', n)\n",
    "        bagging_clf = BaggingClassifier(logistic, max_samples=n, max_features=10, random_state=seed)\n",
    "        bagging_scores = cross_val_score(bagging_clf, X, y, cv=7, n_jobs=-1)\n",
    "\n",
    "        print(\"Mean of: {1:.3f}, std: (+/-) {2:.3f} [Bagging {0}]\\n\".format(svc.__class__.__name__, \n",
    "                            bagging_scores.mean(), bagging_scores.std()))\n",
    "\n",
    "    vanilla_scores = cross_val_score(svc, X, y, cv=7, n_jobs=-1)\n",
    "\n",
    "    print(\"Mean of: {1:.3f}, std: (+/-) {2:.3f} [Vanilla {0}]\\n\".format(svc.__class__.__name__, \n",
    "                            vanilla_scores.mean(), vanilla_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf\n",
      "Accuracy: 0.76\n",
      "\n",
      "########################################\n",
      "                    MR\n",
      "########################################\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "         GENERAL ENGLISH       0.00      0.00      0.00         1\n",
      "      magnetic resonance       0.73      0.96      0.83        28\n",
      "    mitral regurgitation       0.85      0.55      0.67        20\n",
      "myocardial infarction:MI       0.00      0.00      0.00         1\n",
      "\n",
      "               micro avg       0.76      0.76      0.76        50\n",
      "               macro avg       0.39      0.38      0.37        50\n",
      "            weighted avg       0.75      0.76      0.73        50\n",
      "\n",
      "\n",
      "examples (first 5 cases)\t\t\t\t\t\ttrue_abbr\t\t\tpred_abbr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nhttps://github.com/prathamesh1993/Clinical-Acronym-disambiguation\\nhttps://www.datacamp.com/community/tutorials/ensemble-learning-python\\nhttps://scikit-learn.org/stable/modules/ensemble.html\\nhttps://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/\\nhttps://www.datacamp.com/community/tutorials/random-forests-classifier-python\\nhttps://www.springboard.com/blog/beginners-guide-neural-network-in-python-scikit-learn-0-18/\\nhttps://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# valid types in ['svm', 'logistic', 'mlp', 'bagging', 'boosting', rf', 'voting']\n",
    "clf_type = 'rf'\n",
    "seed = 1032\n",
    "\n",
    "# Test: Loop through different abbreviations.\n",
    "# valid abbreevs in ['MR', 'MS', 'MOM', 'FISH', 'OR', 'US']\n",
    "for abbr in ['MR']:\n",
    "\n",
    "    train_abbr = train_grouped_abbr.get_group(abbr)\n",
    "    test_abbr = test_grouped_abbr.get_group(abbr)\n",
    "\n",
    "    X_train = np.array(list(train_abbr.vec))\n",
    "    y_train = train_abbr.expansion\n",
    "\n",
    "    X_test = np.array(list(test_abbr.vec))\n",
    "    y_test = test_abbr.expansion\n",
    "    \n",
    "    if clf_type == 'svm':\n",
    "        # set up SVM\n",
    "        clf = SVC(C=1.0, degree=1, probability=True).fit(X_train, y_train)\n",
    "\n",
    "    elif clf_type == 'logistic':\n",
    "        clf = LogisticRegression().fit(X_train, y_train).fit(X_train, y_train)\n",
    "        \n",
    "    elif clf_type == 'mlp':\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500).fit(X_train, y_train) \n",
    "        \n",
    "    elif clf_type == 'bagging':\n",
    "        clf = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1)).fit(X_train, y_train)\n",
    "        \n",
    "    elif clf_type == 'boosting':\n",
    "        num_trees = 70\n",
    "        clf = AdaBoostClassifier(n_estimators=num_trees, random_state=seed).fit(X_train, y_train)\n",
    "    \n",
    "    elif clf_type == 'rf':\n",
    "        clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "        \n",
    "    elif clf_type == 'voting':\n",
    "        svm = SVC(C=1.0, degree=1, probability=True)\n",
    "        logistic = LogisticRegression()\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "        bagging = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
    "        boosting = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "        rf = RandomForestClassifier()\n",
    "        \n",
    "        estimators = [('svm', svm), ('logistic', logistic), ('mlp', mlp), ('bagging', bagging), ('boosting', boosting), ('rf', rf)]\n",
    "        clf = VotingClassifier(estimators).fit(X_train, y_train)\n",
    "            \n",
    "    pred = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, pred, labels=list(set(df.expansion)))\n",
    "\n",
    "\n",
    "    \n",
    "    print(clf_type)\n",
    "    print('Accuracy:', clf.score(X_test,y_test))\n",
    "    \n",
    "    print()\n",
    "    print(\"##\" * 20)\n",
    "    print(\" \" * 20 + abbr)\n",
    "    print(\"##\" * 20)\n",
    "    \n",
    "    print(classification_report(y_test, pred))\n",
    "    print()\n",
    "    print(f'examples (first 5 cases)\\t\\t\\t\\t\\t\\ttrue_abbr\\t\\t\\tpred_abbr')\n",
    "\n",
    "    # Print first 5 cases\n",
    "    i = 0\n",
    "    for input_row, true_abbr, pred_abbr in zip(train_abbr.iterrows(), y_test, pred):\n",
    "\n",
    "        sn_start = max(input_row[1].start - 25, 0)\n",
    "        sn_end = min(input_row[1].end + 25, len(input_row[1].text))\n",
    "\n",
    "        example_text = input_row[1].text[sn_start: sn_end]\n",
    "        #print(f'... {example_text} ...\\t{true_abbr:<35}\\t{pred_abbr}')\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "'''\n",
    "Refrences:\n",
    "\n",
    "https://github.com/prathamesh1993/Clinical-Acronym-disambiguation\n",
    "https://www.datacamp.com/community/tutorials/ensemble-learning-python\n",
    "https://scikit-learn.org/stable/modules/ensemble.html\n",
    "https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/\n",
    "https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
    "https://www.springboard.com/blog/beginners-guide-neural-network-in-python-scikit-learn-0-18/\n",
    "https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: {}'.format(cross_val_scores))\n",
    "print()\n",
    "#print(set(df.expansion))\n",
    "#print([len(df[df.expansion == x]) for x in set(df.expansion)])\n",
    "print()\n",
    "#print(cm)\n",
    "print()\n",
    "print(f1_score(y_test,pred,average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Patient was tested for US, cystic fibrosis and other heritable diseases '\n",
    "\n",
    "sentence = \"Procedure went without complications, and the patient was sent to the \\\n",
    "floor postoperatively after he was extubated in the FISH. Thoracic surgery was consulted \\\n",
    "on the day of surgery. Gastrografin upper GI study performed on admission showed no leak. \\\n",
    "However, the patient was admitted under the care of thoracic surgery team, and he was kept \\\n",
    "n.p.o. and followed up on daily basis for any change in vital signs, chest pain for another \\\n",
    "upper GI swallow study which was done 7 days after his symptoms started. \"\n",
    "\n",
    "\n",
    "sentence = ['she had an US to determine if the baby was good', 'he had an US to determine if there was a mass', 'If the patient continues to require blood transfusions an/or if he becomes hemodynamically unstable he will need to be taken to the OR for cystoscopy with clot evacuation. Since his surgery was so recently performed we would rather try to hold off on this, however, as not to disrupt the anastomosis with his new transplanted kidney.']\n",
    "\n",
    "for s in sentence:\n",
    "    print('sentence:', s)\n",
    "    #print(vector_breakage(sentence))\n",
    "    print('prediction:', clf.predict(sentence_vector(s).reshape(1, -1)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_out = ''\n",
    "print(vector_breakage(try_out))\n",
    "print(clf.predict(sentence_vector(try_out).reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
