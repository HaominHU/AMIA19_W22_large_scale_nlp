{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_rows = 999\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "glove_input_file = '../models/w2v_glove_300.txt'\n",
    "word2vec_output_file = 'w2v.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "w1 =\"stroke\"\n",
    "print(model.most_similar(positive = w1, topn = 6))\n",
    "#model.most_similar_cosmul(positive=['hepatoma', 'brain'], negative=['liver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "file = \"../data/stopwords.txt\"\n",
    "with open(file) as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def sentence_vector(sentence):\n",
    "    word_list = TreebankWordTokenizer().tokenize(sentence)\n",
    "    word_list = [word for word in word_list if word not in stop_words]\n",
    "    word_vectors = []\n",
    "    for x in word_list:\n",
    "        try:\n",
    "            word_vectors.append(model[x])\n",
    "        except KeyError:\n",
    "            None    \n",
    "    return sum(word_vectors)/len(word_vectors)\n",
    "\n",
    "def vector_breakage(sentence):\n",
    "    word_list = TreebankWordTokenizer().tokenize(sentence)\n",
    "    word_list = [word for word in word_list if word not in stop_words]\n",
    "    word_vectors_list = []\n",
    "    for x in word_list:\n",
    "        try:\n",
    "            if len(model[x])==200:\n",
    "                word_vectors_list.append(x)\n",
    "        except:\n",
    "            None\n",
    "        else:\n",
    "            None\n",
    "    return word_vectors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# load prepartitioned train/test sets\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "train = pd.read_csv(\"../data/AMIA_train_set.csv\")\n",
    "\n",
    "#print(train)\n",
    "\n",
    "# load full data set\n",
    "frames = [test, train]\n",
    "df = pd.concat(frames)\n",
    "df = df[['text','expansion']]\n",
    "df['vec'] = [sentence_vector(x) for x in df.text]\n",
    "df.expansion.unique()\n",
    "\n",
    "test = test[['text','expansion', 'case']]\n",
    "train = train[['text','expansion']]\n",
    "test['vec'] = [sentence_vector(x) for x in test.text]\n",
    "train['vec'] = [sentence_vector(x) for x in train.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X = list(df.vec)\n",
    "X = np.array(X)\n",
    "y = df.expansion\n",
    "\n",
    "X1 = list(train.vec)\n",
    "X_train = np.array(X1)\n",
    "y_train = train.expansion\n",
    "\n",
    "X2 = list(test.vec)\n",
    "X_test = np.array(X2)\n",
    "y_test = test.expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "clf = SVC(C=1.0, kernel='linear', degree=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, pred,labels=list(set(df.expansion)))\n",
    "cross_val_scores = cross_val_score(clf, X, y, cv=7)\n",
    "\n",
    "predicted_expansion = list(pred)\n",
    "case = test['case'].tolist()\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    {'case': case,\n",
    "     'expansion': predicted_expansion\n",
    "    })\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print('accuracy: {}'.format(cross_val_scores))\n",
    "print()\n",
    "#print(set(df.expansion))\n",
    "#print([len(df[df.expansion == x]) for x in set(df.expansion)])\n",
    "print()\n",
    "#print(cm)\n",
    "print()\n",
    "print(f1_score(y_test,pred,average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sentence = 'Patient was tested for US, cystic fibrosis and other heritable diseases '\n",
    "\n",
    "sentence = \"Procedure went without complications, and the patient was sent to the \\\n",
    "floor postoperatively after he was extubated in the FISH. Thoracic surgery was consulted \\\n",
    "on the day of surgery. Gastrografin upper GI study performed on admission showed no leak. \\\n",
    "However, the patient was admitted under the care of thoracic surgery team, and he was kept \\\n",
    "n.p.o. and followed up on daily basis for any change in vital signs, chest pain for another \\\n",
    "upper GI swallow study which was done 7 days after his symptoms started. \"\n",
    "\n",
    "\n",
    "sentence = ['she had an US to determine if the baby was good', 'he had an US to determine if there was a mass', 'If the patient continues to require blood transfusions an/or if he becomes hemodynamically unstable he will need to be taken to the OR for cystoscopy with clot evacuation. Since his surgery was so recently performed we would rather try to hold off on this, however, as not to disrupt the anastomosis with his new transplanted kidney.']\n",
    "\n",
    "for s in sentence:\n",
    "    print('sentence:', s)\n",
    "    #print(vector_breakage(sentence))\n",
    "    print('prediction:', clf.predict(sentence_vector(s).reshape(1, -1)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "try_out = ''\n",
    "print(vector_breakage(try_out))\n",
    "print(clf.predict(sentence_vector(try_out).reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}