{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_rows = 999\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = '/Users/gms/development/nlp/nlpie/projects/acronym/AMIA19_W22_large_scale_nlp/models/w2v_glove_300.txt'\n",
    "word2vec_output_file = 'w2v.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('heart', 0.4828336238861084), ('cardiac', 0.45815250277519226), ('debilitating', 0.4523412585258484), ('infarction', 0.4387247562408447), ('illness', 0.4336370527744293), ('complications', 0.43345579504966736)]\n"
     ]
    }
   ],
   "source": [
    "w1 =\"stroke\"\n",
    "print(model.most_similar(positive = w1, topn = 6))\n",
    "#model.most_similar_cosmul(positive=['hepatoma', 'brain'], negative=['liver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "file = \"/Users/gms/development/nlp/nlpie/projects/acronym/AMIA19_W22_large_scale_nlp/data/stopwords.txt\"\n",
    "with open(file) as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "def sentence_vector(sentence):\n",
    "    word_list = TreebankWordTokenizer().tokenize(sentence)\n",
    "    #stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    #word_list = [stemmer.stem(x) for x in word_list]\n",
    "    word_list = [word for word in word_list if word not in stop_words]\n",
    "    word_vectors = []\n",
    "    for x in word_list:\n",
    "        try:\n",
    "            word_vectors.append(model[x])\n",
    "        except KeyError:\n",
    "            None    \n",
    "    return sum(word_vectors)/len(word_vectors)\n",
    "def vector_breakage(sentence):\n",
    "    word_list = TreebankWordTokenizer().tokenize(sentence)\n",
    "    #stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    #word_list = [stemmer.stem(x) for x in word_list]\n",
    "    word_list = [word for word in word_list if word not in stop_words]\n",
    "    word_vectors_list = []\n",
    "    for x in word_list:\n",
    "        try:\n",
    "            if len(model[x])==200:\n",
    "                word_vectors_list.append(x)\n",
    "        except:\n",
    "            None\n",
    "        else:\n",
    "            None\n",
    "    return word_vectors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full data set\n",
    "df = pd.read_csv(\"/Users/gms/development/nlp/nlpie/projects/acronym/AMIA19_W22_large_scale_nlp/data/full.csv\")\n",
    "df = df[['text','expansion']]\n",
    "df['vec'] = [sentence_vector(x) for x in df.text]\n",
    "df.expansion.unique()\n",
    "\n",
    "\n",
    "# load prepartitioned train/test sets\n",
    "test = pd.read_csv(\"/Users/gms/development/nlp/nlpie/projects/acronym/AMIA19_W22_large_scale_nlp/data/test.csv\")\n",
    "test = test[['text','expansion']]\n",
    "test['vec'] = [sentence_vector(x) for x in test.text]\n",
    "\n",
    "train = pd.read_csv(\"/Users/gms/development/nlp/nlpie/projects/acronym/AMIA19_W22_large_scale_nlp/data/test.csv\")\n",
    "train = train[['text','expansion']]\n",
    "train['vec'] = [sentence_vector(x) for x in train.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(train.vec)\n",
    "X_train = np.array(X)\n",
    "y_train = train.expansion\n",
    "\n",
    "X = list(test.vec)\n",
    "X_test = np.array(X)\n",
    "y_test = test.expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=1.0, kernel='linear', degree=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=7.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, pred,labels=list(set(df.expansion)))\n",
    "cross_val_scores = cross_val_score(clf, X, y, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: [0.61666667 0.62962963 0.54901961 0.58333333 0.65957447 0.65217391\n",
      " 0.61363636]\n",
      "\n",
      "{'morphine sulfate', 'Mall of America:MOA', 'intrathecal', 'mental retardation', 'intertrochanteric', 'immature-to-total neutrophil', 'myocardial infarction:MI', 'UNSURED SENSE', 'United States', '(drug) OR', 'fluorescent in situ hybridization', 'magnetic resonance', 'idiopathic thrombocytopenic purpura:ITP', 'multiples of median', 'mitral stenosis', 'information technology', 'multiple sclerosis', '(drug) MS', 'ischial tuberosity', 'pravastatin or atorvastatin evaluation and infection therapy', 'iliotibial', 'medical student', '(drug) IT', 'milk of magnesia', 'operating room', 'ultrasound', 'master of science', 'GENERAL ENGLISH', 'mitral regurgitation', 'musculoskeletal', 'inspiratory time', 'medical record'}\n",
      "[279, 1, 58, 3, 14, 1, 1, 11, 396, 1, 449, 314, 1, 439, 1, 102, 207, 1, 48, 1, 40, 1, 2, 57, 466, 94, 3, 329, 176, 2, 1, 1]\n",
      "\n",
      "[[29  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0 41  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 44  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 28  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 43  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
      "   2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 3  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2\n",
      "   0  0  0  1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  46  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  6  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0 24  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0 19  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n",
      "\n",
      "0.930040276338676\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: {}'.format(cross_val_scores))\n",
    "print()\n",
    "print(set(df.expansion))\n",
    "print([len(df[df.expansion == x]) for x in set(df.expansion)])\n",
    "print()\n",
    "print(cm)\n",
    "print()\n",
    "print(f1_score(y_test,pred,average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['information technology']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Patient was tested for US, cystic fibrosis and other heritable diseases '\n",
    "'''\n",
    "sentence = \"Procedure went without complications, and the patient was sent to the \\\n",
    "floor postoperatively after he was extubated in the MM. Thoracic surgery was consulted \\\n",
    "on the day of surgery. Gastrografin upper GI study performed on admission showed no leak. \\\n",
    "However, the patient was admitted under the care of thoracic surgery team, and he was kept \\\n",
    "n.p.o. and followed up on daily basis for any change in vital signs, chest pain for another \\\n",
    "upper GI swallow study which was done 7 days after his symptoms started. \"\n",
    "'''\n",
    "\n",
    "print(vector_breakage(sentence))\n",
    "print(clf.predict(sentence_vector(sentence).reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e3ee18af3c25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtry_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_breakage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtry_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtry_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-b6cf5e47f2d8>\u001b[0m in \u001b[0;36msentence_vector\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvector_breakage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "try_out = ''\n",
    "print(vector_breakage(try_out))\n",
    "print(clf.predict(sentence_vector(try_out).reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
